# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_z0wh_JrdhUbgtTN0l3hKU-OnW6R5OiY
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split

# Load IMDb dataset with a limit of 1000 samples
max_samples = 1000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000, maxlen=max_samples)

# Pad sequences to a fixed length
max_len = 200
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)

# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Print information about the dataset
print(f"Number of training samples: {len(x_train)}")
print(f"Number of validation samples: {len(x_val)}")
print(f"Number of testing samples: {len(x_test)}")
print(f"Max length of sequences: {max_len}")

# Build the model
model = Sequential([
    Embedding(input_dim=5000, output_dim=16, input_length=max_len),
    LSTM(64),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print model summary
model.summary()

# Train the model
history = model.fit(
    x_train, y_train,
    epochs=5,
    batch_size=128,
    validation_data=(x_val, y_val)
)

# Evaluate the model
train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)
val_loss, val_acc = model.evaluate(x_val, y_val, verbose=0)

print(f'Training Accuracy: {train_acc * 100:.2f}%')
print(f'Validation Accuracy: {val_acc * 100:.2f}%')